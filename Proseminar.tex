\documentclass[a4paper, 11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}
\usepackage{lmodern}
\usepackage{hyphenat}
\usepackage[babel,german=quotes]{csquotes}
\usepackage[left=1cm,top=1cm,right=1cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx,overpic}
\usepackage{pdflscape}
\usepackage{verbatim}
\usepackage{mdwlist} %less space for lists
\usepackage[hidelinks,pdfencoding=auto]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Proseminar Objektposenschätzung}
    }
\usepackage{multicol}
%\setlength{\columnseprule}{1pt}
%\setlength{\columnsep}{1cm}
\setlength{\parskip}{0pt}
\setlength{\parindent}{0pt}

\usepackage[style=alphabetic]{biblatex}
\setlength{\bibitemsep}{1em} 
\bibliography{Proseminar}

\usepackage{glossaries}
\setglossarystyle{list} 
\makeglossaries
\newglossaryentry{cnn}{
    name=Convolutional Neural Network,
    description={Besitzt pro Convolutional Layer mehrere Filterkerne, sodass Schichten an Feature Maps entstehen, die jeweils die gleiche Eingabe bekommen, jedoch aufgrund unterschiedlicher Gewichtsmatrizen unterschiedliche Features extrahieren.}
}
\newglossaryentry{quaternion}{
    name={Quaternion-Darstellung},
    description={Darstellung $V$ einer Gruppe $G$, die einen $G$-invarianten Homomorphismus $J:V\rightarrow V$ besitzt, der antilinear ist und $J^2=-Id$ erfüllt.}
    }

\title{Proseminar Objektposenschätzung}
\author{Robert Jeutter}
\date{\today}
\pdfinfo{
    /Title (Proseminar Objektposenschätzung)
    /Creator (TeX)
    /Producer (pdfTeX 1.40.0)
    /Author (Robert Jeutter)
    /Subject (Deeplearning, Robotische Manipulation)
}

\begin{document}
\maketitle

\begin{multicols*}{2}
    \textbf{
        Damit ein Roboter einen Gegenstand greifen kann, ist es meist notwendig die genaue Lage des Objektes zu kennen. Dies kann sowohl über klassische Verfahren als auch über Deep-Learning-Verfahren erreicht werden. Ziel dieses Seminars ist es den Stand der Technik für die Objektposenschätzung aufzuarbeiten und vorzustellen. Der Fokus sollte dabei auf Verfahren liegen, bei denen zuvor kein Objektmodell benötig wird, sodass auch die Lage von unbekannten Objekten geschätzt werden kann.
    }

    \section{Motivation}
    %Die Schätzung der 6D-Position bekannter Objekte ist wichtig für die Interaktion von Robotern mit der realen Welt. 
    Die Erkennung von Objekten und die Schätzung ihrer Lage in 3D hat eine Vielzahl von Anwendungen in der Robotik. So ist beispielsweise die Erkennung der 3D-Lage und Ausrichtung von Objekten wichtig für die Robotermanipulation. Sie ist auch bei Aufgaben der Mensch-Roboter-Interaktion nützlich, z. B. beim Lernen aus Demonstrationen.
    %Für Robotermanipulation sind häufig Informationen über die Position des manipulierten Objekts erforderlich. 
    %In einigen Fällen kann dies durch Vorwärtskinematik erreicht werden, wobei angenommen wird, dass die Bewegung des Objekts der Bewegung des Endeffektors entspricht. Häufig reicht die Vorwärtskinematik jedoch nicht aus, um die Lage des Objekts genau zu bestimmen. Dies kann durch Schlupf beim Greifen oder bei der Manipulation mit der Hand, bei der Übergabe oder durch die Nachgiebigkeit eines Saugnapfes bedingt sein. In diesen Fällen ist eine dynamische Schätzung der Objektposition aus visuellen Daten wünschenswert.

    %Methoden zur 6D-Positionsschätzung aus Einzelbildern wurden ausgiebig untersucht. Einige von ihnen sind schnell und können die Pose für jedes neue Bild von Grund auf neu schätzen. Dies ist jedoch redundant, weniger effizient und führt zu weniger kohärenten Schätzungen für aufeinanderfolgende Bilder. Andererseits kann die Verfolgung von 6D-Objektposen über Bildsequenzen bei einer anfänglichen Posenschätzung die Schätzgeschwindigkeit verbessern und gleichzeitig kohärente und genaue Posen liefern, indem die zeitliche Konsistenz genutzt wird.
    Traditionell wird das Problem der Objektposenschätzung durch den Abgleich von Merkmalspunkten zwischen 3D-Modellen und Bildern angegangen. Diese Methoden setzen jedoch voraus, dass die Objekte reichhaltig texturiert sind, um Merkmalspunkte für den Abgleich zu erkennen. Daher sind sie nicht in der Lage, mit Objekten ohne Textur umzugehen. %Bei merkmalsbasierten Methoden werden lokale Merkmale entweder aus Points of Interest oder aus jedem Pixel des Bildes extrahiert und mit den Merkmalen der 3D-Modelle abgeglichen, um 2D-3D-Korrespondenzen herzustellen, aus denen 6D-Posen wiederhergestellt werden können. Merkmalsbasierte Methoden sind in der Lage, mit Verdeckungen zwischen Objekten umzugehen. Sie benötigen jedoch ausreichende Texturen auf den Objekten, um die lokalen Merkmale berechnen zu können. Um mit texturlosen Objekten umzugehen, wurden mehrere Methoden vorgeschlagen, um Merkmalsdeskriptoren mit Hilfe von maschinellen Lernverfahren zu lernen. Einige Ansätze wurden vorgeschlagen, um direkt auf die 3D-Objektkoordinaten jedes Pixels zu regressieren, um die 2D-3D-Korrespondenzen herzustellen. Die 3D-Koordinatenregression stößt jedoch bei symmetrischen Objekten auf Mehrdeutigkeiten.
    Die meisten bestehenden Ansätze zur Objektposenschätzung setzen den Zugriff auf das 3D-Modell einer Objektinstanz voraus. Der Zugang zu solchen 3D-Modellen erschwert die Verallgemeinerung auf neue, unbekannte Instanzen. %Um diese Einschränkung zu überwinden, haben neuere Ansätze diese Annahme gelockert und benötigen nur 3D-Modelle auf Kategorieebene für die 6D-Positionsschätzung. Dies wird häufig durch Training mit einer großen Anzahl von CAD-Modellen derselben Kategorie erreicht. Obwohl für bereits bekannte Objektkategorien vielversprechende Ergebnisse erzielt wurden, gibt es immer noch Einschränkungen z.B. durch die gerine Vielfalt von Kategorien. Beliebte 3D-Modelldatenbanken wie ShapeNet\footnote{\href{https://shapenet.org/}{shapenet.org}} und ModelNet40\footnote{\href{https://modelnet.cs.princeton.edu/}{modelnet.cs.princeton.edu}} enthalten 55 bzw. 40 Kategorien. Dies reicht bei weitem nicht aus, um die verschiedenen Objektkategorien in der realen Welt abzudecken. 
    Darüber hinaus erfordern 3D-Modelldatenbanken oft einen nicht unerheblichen manuellen Aufwand und Expertenwissen, um sie zu erstellen, wobei Schritte wie Scannen, Netzverfeinerung oder CAD-Design erforderlich sind.
    Unordnung und Verdeckungen zwischen den Objekten senken die korrekte Erkennung bei modellbasierten Verfahren zudem deutlich.
    Bei schablonenbasierten Methoden wird eine starre Schablone konstruiert und verwendet, um verschiedene Stellen im Eingabebild zu scannen. An jeder Stelle wird ein Ähnlichkeitswert berechnet, und die beste Übereinstimmung wird durch den Vergleich dieser Ähnlichkeitswerte ermittelt.
    Schablonenbasierte Methoden sind nützlich für die Erkennung texturloser Objekte. Sie können jedoch nicht sehr gut mit Verdeckungen zwischen Objekten umgehen, da die Vorlage einen niedrigen Ähnlichkeitswert hat, wenn das Objekt verdeckt ist.
    Alternativ dazu können Methoden, die eine Regression von Bildpixeln auf 3D-Objektkoordinaten erlernen, um 2D-3D-Korrespondenzen für die 6D-Positionsschätzung herzustellen, nicht mit symmetrischen Objekten umgehen.
    Außerdem können bei der Verfolgung durch solche dynamische on-the-fly Rekonstruktion von Objekten Fehler entstehen, wenn Beobachtungen mit fehlerhaften Posenschätzungen in das globale Modell einfließen. Diese Fehler wirken sich nachteilig auf die Modellverfolgung in nachfolgenden Bildern aus.

    \section{Kategorisierung}
    Motiviert durch die oben genannten Einschränkungen, zielt diese Arbeit auf eine genaue, robuste 6D-Objekterkennung ab, die auf neuartige Objekte ohne 3D-Modelle verallgemeinert werden kann.
    Die Kategorisierung aller Verfahren erfolgt nach folgendem Schemata
    \begin{description*}
        \item[Modell] Müssen für das Training oder Nutzung merkmalsbasierte oder Objektmodelle (2D, 3D, CAD) vorhanden sein?
        \item[Video-Input] Verarbeitet das Verfahren 2D Bilder, 3D Bilder mit Tiefenwahrnehmung?
        \item[Datensatz] mit welchen Datensätzen wurde das Verfahren trainiert oder getestet?
        \item[Genauigkeit] Wie akkurat ist die Objektposenschätzung im Vergleich?
        \item[Ressourcen] Wie Ressourcenintensiv ist das Verfahren? Wird spezielle Hardware benötigt?
        \item[Laufzeit] Mit welcher Geschwindigkeit ist die Verarbeitung von Eingabedaten möglich und stabil?
    \end{description*}

    \section{Verschiedene Verfahren}
    \subsection{BundleTrack\cite{BundleTrack}}
    \begin{description*}
        \item[Modell]
        \item[Video-Input]
        \item[Datensatz]
        \item[Genauigkeit]
        \item[Ressourcen]
        \item[Laufzeit]
    \end{description*}

    \subsection{DeepIM\cite{Deepim}}
    \begin{description*}
        \item[Modell]
        \item[Video-Input]
        \item[Datensatz]
        \item[Genauigkeit]
        \item[Ressourcen]
        \item[Laufzeit]
    \end{description*}

    \subsection{MaskFusion\cite{MaskFusion}}
    \begin{description*}
        \item[Modell]
        \item[Video-Input]
        \item[Datensatz]
        \item[Genauigkeit]
        \item[Ressourcen]
        \item[Laufzeit]
    \end{description*}

    \subsection{Neural Analysis-by-Synthesis\cite{CategoryLevelObject}}
    \begin{description*}
        \item[Modell]
        \item[Video-Input]
        \item[Datensatz]
        \item[Genauigkeit]
        \item[Ressourcen]
        \item[Laufzeit]
    \end{description*}

    \subsection{6-PACK\cite{6pack}}
    \begin{description*}
        \item[Modell]
        \item[Video-Input]
        \item[Datensatz]
        \item[Genauigkeit]
        \item[Ressourcen]
        \item[Laufzeit]
    \end{description*}

    \subsection{PoseCNN\cite{PoseCNN}}
    \begin{description*}
        \item[Modell]
        \item[Video-Input]
        \item[Datensatz]
        \item[Genauigkeit]
        \item[Ressourcen]
        \item[Laufzeit]
    \end{description*}

    \subsection{Robust Gaussian Filter\cite{GaussianFilter}}
    \begin{description*}
        \item[Modell]
        \item[Video-Input]
        \item[Datensatz]
        \item[Genauigkeit]
        \item[Ressourcen]
        \item[Laufzeit]
    \end{description*}

    \section{Vergleich verschiedener Verfahren}

    \section{Fazit}

\end{multicols*}

\medskip

\printglossary[title=Glossar]

\printbibliography[title=Literatur]

\end{document}