\documentclass[a4paper, 11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}
\usepackage{lmodern}
\usepackage{hyphenat}
\usepackage[babel,german=quotes]{csquotes}
\usepackage[left=1cm,top=1cm,right=1cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx,overpic}
\usepackage{pdflscape}
\usepackage{verbatim}
\usepackage{mdwlist} %less space for lists
\usepackage{gensymb} %degree symbol
\usepackage[hidelinks,pdfencoding=auto]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Proseminar Objektposenschätzung}
    }
\usepackage{multicol}
%\setlength{\columnseprule}{1pt}
%\setlength{\columnsep}{1cm}
\setlength{\parskip}{0pt}
\setlength{\parindent}{0pt}

\usepackage[style=alphabetic]{biblatex}
\setlength{\bibitemsep}{1em} 
\bibliography{Proseminar}

\usepackage{glossaries}
\setglossarystyle{list} 
\makeglossaries
\newglossaryentry{cnn}{
    name=Convolutional Neural Network,
    description={Besitzt pro Convolutional Layer mehrere Filterkerne, sodass Schichten an Feature Maps entstehen, die jeweils die gleiche Eingabe bekommen, jedoch aufgrund unterschiedlicher Gewichtsmatrizen unterschiedliche Features extrahieren.}
}
\newglossaryentry{quaternion}{
    name={Quaternion-Darstellung},
    description={Darstellung $V$ einer Gruppe $G$, die einen $G$-invarianten Homomorphismus $J:V\rightarrow V$ besitzt, der antilinear ist und $J^2=-Id$ erfüllt.}
    }
\newglossaryentry{NOCS}{
    name={NOCS},
    description={Der Datensatz enthält 6 Objektkategorien: Flasche, Schüssel, Kamera, Dose, Laptop und Becher. Der Trainingssatz besteht aus: 7 realen Videos mit insgesamt 3 Instanzen jeder Kategorie, die mit Grundwahrheits-Posen annotiert sind; und 275K Frames synthetischer Daten, die mit 1085 Instanzen aus den oben genannten 6 Kategorien unter Verwendung einer 3D-Modell-Datenbank ShapeNetCore mit zufälligen Posen und Objektkombinationen in jeder Szene erzeugt wurden. Der Testsatz besteht aus 6 realen Videos, die 3 verschiedene ungesehene Instanzen innerhalb jeder Kategorie enthalten, was zu 18 verschiedenen Objektinstanzen und insgesamt 3 Frames führt.}
    }
\newglossaryentry{YCBInEOAT}{
    name={YCBInEOAT},
    description={Dieser Datensatz hilft, die Effektivität der 6D-Positionsverfolgung während der Robotermanipulation zu überprüfen. Er wurde ursprünglich entwickelt, um Ansätze zu bewerten, die auf CAD-Modellen basieren. Im Gegensatz zum NOCS-Datensatz, bei dem die Objekte statisch auf einer Tischplatte platziert und von einer beweglichen Kamera erfasst werden, enthält YCBInEOAT 9 Videosequenzen, die von einer statischen RGB-D-Kamera aufgenommen wurden, während die Objekte dynamisch manipuliert werden. Es gibt drei Arten der Manipulation: (1) einarmiges Greifen und Platzieren, (2) Manipulation innerhalb der Hand und (3) Greifen und Übergeben zwischen den Armen zur Platzierung. Diese Szenarien und die verwendeten Endeffektoren machen die direkte Berechnung von posen aus Vorwärtskinematik unzuverlässig. Die Videos beinhalten 5 YCB Objekte: Senfglas, Tomatensuppendose, Zuckerbox, Bleichereiniger und eine Keksbox.}}
\newglossaryentry{55cm}{
    name={5\textdegree 5cm},
    description={Prozentsatz der Schätzungen mit einem Orientierungsfehler $<5$\textdegree und einem Translationsfehler $< 5cm$ - je höher, desto besser}
}
\newglossaryentry{IoU25}{
    name={IoU25},
    description={(Intersection over Union) Prozentualer Anteil der Fälle, in denen die Überschneidung von Vorhersage und 3D Bounding Box größer ist als 25\% ihrer Vereinigung - je höher, desto besser
    besser}
}
\newglossaryentry{Rerr}{
    name={R\_err},%R_{err}
    description={mittlerer Orientierungsfehler in Grad - je geringer desto besser}
}
\newglossaryentry{Terr}{
    name={T\_err},%T_{err}
    description={mittlerer Übersetzungsfehler in Zentimetern - je niedriger, desto besser}
}
\newglossaryentry{AUC}{
    name={AUC},
    description={Area Under Curve: Die Ergebnisse werden anhand der Genauigkeitsschwelle AUC berechnet, die von ADD gemessen wird, das einen exakten Modellabgleich durchführt, und ADD-S, das für die Bewertung symmetrischer Objekte konzipiert ist.}
}

\title{Proseminar Objektposenschätzung}
\author{Robert Jeutter}
\date{\today}
\pdfinfo{
    /Title (Proseminar Objektposenschätzung)
    /Creator (TeX)
    /Producer (pdfTeX 1.40.0)
    /Author (Robert Jeutter)
    /Subject (Deeplearning, Robotische Manipulation)
}

\begin{document}
\maketitle

\begin{multicols*}{2}
    \textbf{
        Damit ein Roboter einen Gegenstand greifen kann, ist es meist notwendig die genaue Lage des Objektes zu kennen. Dies kann sowohl über klassische Verfahren als auch über Deep-Learning-Verfahren erreicht werden. Ziel dieses Seminars ist es den Stand der Technik für die Objektposenschätzung aufzuarbeiten und vorzustellen. Der Fokus sollte dabei auf Verfahren liegen, bei denen zuvor kein Objektmodell benötig wird, sodass auch die Lage von unbekannten Objekten geschätzt werden kann.
    }

    \section{Motivation}
    %Die Schätzung der 6D-Position bekannter Objekte ist wichtig für die Interaktion von Robotern mit der realen Welt. 
    Die Erkennung von Objekten und die Schätzung ihrer Lage in 3D hat eine Vielzahl von Anwendungen in der Robotik. So ist beispielsweise die Erkennung der 3D-Lage und Ausrichtung von Objekten wichtig für die Robotermanipulation. Sie ist auch bei Aufgaben der Mensch-Roboter-Interaktion nützlich, z. B. beim Lernen aus Demonstrationen.
    %Für Robotermanipulation sind häufig Informationen über die Position des manipulierten Objekts erforderlich. 
    %In einigen Fällen kann dies durch Vorwärtskinematik erreicht werden, wobei angenommen wird, dass die Bewegung des Objekts der Bewegung des Endeffektors entspricht. Häufig reicht die Vorwärtskinematik jedoch nicht aus, um die Lage des Objekts genau zu bestimmen. Dies kann durch Schlupf beim Greifen oder bei der Manipulation mit der Hand, bei der Übergabe oder durch die Nachgiebigkeit eines Saugnapfes bedingt sein. In diesen Fällen ist eine dynamische Schätzung der Objektposition aus visuellen Daten wünschenswert.

    %Methoden zur 6D-Positionsschätzung aus Einzelbildern wurden ausgiebig untersucht. Einige von ihnen sind schnell und können die Pose für jedes neue Bild von Grund auf neu schätzen. Dies ist jedoch redundant, weniger effizient und führt zu weniger kohärenten Schätzungen für aufeinanderfolgende Bilder. Andererseits kann die Verfolgung von 6D-Objektposen über Bildsequenzen bei einer anfänglichen Posenschätzung die Schätzgeschwindigkeit verbessern und gleichzeitig kohärente und genaue Posen liefern, indem die zeitliche Konsistenz genutzt wird.
    Traditionell wird das Problem der Objektposenschätzung durch den Abgleich von Merkmalspunkten zwischen 3D-Modellen und Bildern angegangen. Diese Methoden setzen jedoch voraus, dass die Objekte reichhaltig texturiert sind, um Merkmalspunkte für den Abgleich zu erkennen. Daher sind sie nicht in der Lage, mit Objekten ohne Textur umzugehen. %Bei merkmalsbasierten Methoden werden lokale Merkmale entweder aus Points of Interest oder aus jedem Pixel des Bildes extrahiert und mit den Merkmalen der 3D-Modelle abgeglichen, um 2D-3D-Korrespondenzen herzustellen, aus denen 6D-Posen wiederhergestellt werden können. Merkmalsbasierte Methoden sind in der Lage, mit Verdeckungen zwischen Objekten umzugehen. Sie benötigen jedoch ausreichende Texturen auf den Objekten, um die lokalen Merkmale berechnen zu können. Um mit texturlosen Objekten umzugehen, wurden mehrere Methoden vorgeschlagen, um Merkmalsdeskriptoren mit Hilfe von maschinellen Lernverfahren zu lernen. Einige Ansätze wurden vorgeschlagen, um direkt auf die 3D-Objektkoordinaten jedes Pixels zu regressieren, um die 2D-3D-Korrespondenzen herzustellen. Die 3D-Koordinatenregression stößt jedoch bei symmetrischen Objekten auf Mehrdeutigkeiten.
    Die meisten bestehenden Ansätze zur Objektposenschätzung setzen den Zugriff auf das 3D-Modell einer Objektinstanz voraus. Der Zugang zu solchen 3D-Modellen erschwert die Verallgemeinerung auf neue, unbekannte Instanzen. %Um diese Einschränkung zu überwinden, haben neuere Ansätze diese Annahme gelockert und benötigen nur 3D-Modelle auf Kategorieebene für die 6D-Positionsschätzung. Dies wird häufig durch Training mit einer großen Anzahl von CAD-Modellen derselben Kategorie erreicht. Obwohl für bereits bekannte Objektkategorien vielversprechende Ergebnisse erzielt wurden, gibt es immer noch Einschränkungen z.B. durch die gerine Vielfalt von Kategorien. Beliebte 3D-Modelldatenbanken wie ShapeNet\footnote{\href{https://shapenet.org/}{shapenet.org}} und ModelNet40\footnote{\href{https://modelnet.cs.princeton.edu/}{modelnet.cs.princeton.edu}} enthalten 55 bzw. 40 Kategorien. Dies reicht bei weitem nicht aus, um die verschiedenen Objektkategorien in der realen Welt abzudecken. 
    Darüber hinaus erfordern 3D-Modelldatenbanken oft einen nicht unerheblichen manuellen Aufwand und Expertenwissen, um sie zu erstellen, wobei Schritte wie Scannen, Netzverfeinerung oder CAD-Design erforderlich sind.
    Unordnung und Verdeckungen zwischen den Objekten senken die korrekte Erkennung bei modellbasierten Verfahren zudem deutlich.
    Bei schablonenbasierten Methoden wird eine starre Schablone konstruiert und verwendet, um verschiedene Stellen im Eingabebild zu scannen. An jeder Stelle wird ein Ähnlichkeitswert berechnet, und die beste Übereinstimmung wird durch den Vergleich dieser Ähnlichkeitswerte ermittelt.
    Schablonenbasierte Methoden sind nützlich für die Erkennung texturloser Objekte. Sie können jedoch nicht sehr gut mit Verdeckungen zwischen Objekten umgehen, da die Vorlage einen niedrigen Ähnlichkeitswert hat, wenn das Objekt verdeckt ist.
    Alternativ dazu können Methoden, die eine Regression von Bildpixeln auf 3D-Objektkoordinaten erlernen, um 2D-3D-Korrespondenzen für die 6D-Positionsschätzung herzustellen, nicht mit symmetrischen Objekten umgehen.
    Außerdem können bei der Verfolgung durch solche dynamische on-the-fly Rekonstruktion von Objekten Fehler entstehen, wenn Beobachtungen mit fehlerhaften Posenschätzungen in das globale Modell einfließen. Diese Fehler wirken sich nachteilig auf die Modellverfolgung in nachfolgenden Bildern aus.

    \section{Kategorisierung}
    Motiviert durch die oben genannten Einschränkungen, zielt diese Arbeit auf eine genaue, robuste 6D-Objekterkennung ab, die auf neuartige Objekte ohne 3D-Modelle verallgemeinert werden kann.
    Die Kategorisierung aller Verfahren erfolgt nach folgendem Schemata
    \begin{description*}
        \item[Modell] Müssen für das Training oder Nutzung merkmalsbasierte oder Objektmodelle (2D, 3D, CAD) vorhanden sein?
        \item[Video-Input] Verarbeitet das Verfahren 2D Bilder, 3D Bilder mit Tiefenwahrnehmung?
        \item[Datensatz] mit welchen Datensätzen wurde das Verfahren trainiert oder getestet?
        \item[Genauigkeit] Wie akkurat ist die Objektposenschätzung im Vergleich?
        \item[Ressourcen] Wie Ressourcenintensiv ist das Verfahren? Wird spezielle Hardware benötigt?
        \item[Laufzeit] Mit welcher Geschwindigkeit ist die Verarbeitung von Eingabedaten möglich und stabil?
    \end{description*}

    \section{Verschiedene Verfahren}
    \subsection{BundleTrack}
    BundleTrack\cite{BundleTrack} ist ein Framework für die 6D-Positionsverfolgung neuartiger Objekte, das nicht von 3D-Modellen auf Instanz- oder Kategorieebene abhängt. Es nutzt komplementären Eigenschaften für die Segmentierung und robuste Merkmalsextraktion sowie die speichererweiterte Pose-Graph-Optimierung für die räumlich-zeitliche Konsistenz. Dies ermöglicht eine langfristige, abdriftarme Verfolgung in verschiedenen anspruchsvollen Szenarien, einschließlich erheblicher Verdeckungen und Objektbewegungen.

    Im Vergleich zu modernen Methoden, die auf einem CAD-Modell der Objektinstanz basieren, wird eine vergleichbare Leistung erzielt, obwohl die vorgeschlagene Methode weniger Informationen benötigt. Eine effiziente Implementierung in CUDA ermöglicht eine Echtzeitleistung von 10 Hz für das gesamte System.
    Der Code ist verfügbar unter: \href{https://github.com/wenbowen123/BundleTrack}{github.com/wenbowen123/BundleTrack}

    \begin{description*}
        \item[Modell] ohne Modelle
        \item[Video-Input] RGB-D
        \item[Datensatz] \Gls{NOCS}, \Gls{YCBInEOAT}, Davis\cite{Davis}, Youtube-VOS\cite{Youtube-vos}
        \item[Genauigkeit] kann mit Verdeckung und Objektbewegung gut umgehen. Vergleichbare Leistung mit Methoden mit CAD Modell. Im \Gls{NOCS}-Datensatz:
        \begin{itemize*}
            \item $87,4\%$ \Gls{55cm}
            \item $99,9\%$ \Gls{IoU25}
            \item $R_{err}=2,4$
            \item $T_{err}=2,1$
        \end{itemize*}
        Ergebnisse aus \Gls{AUC} Messung
        \begin{itemize*}
            \item ADD $87,34\%$
            \item ADD-S $92,53\%$
        \end{itemize*}
        \item[Ressourcen] effiziente CUDA-Implementierung, ermöglicht es, das rechenintensive Multi-Pair-Feature-Matching sowie die Pose-Graph-Optimierung für die 6D-Objekt-Positionsverfolgung online auszuführen. Alle Experimente wurden auf einem Standard-Desktop mit Intel Xeon(R) E5-1660 v3@3.00GHz Prozessor und einer einzelnen NVIDIA RTX 2080 Ti GPU durchgeführt.
        \item[Laufzeit] in CUDA Echtzeit mit 10 Hz
    \end{description*}

    \subsection{DeepIM}\cite{Deepim}
    \begin{description*}
        \item[Modell]
        \item[Video-Input]
        \item[Datensatz]
        \item[Genauigkeit]
        \item[Ressourcen]
        \item[Laufzeit]
    \end{description*}

    \subsection{MaskFusion}\cite{MaskFusion}
    \begin{description*}
        \item[Modell] ohne Modell
        \item[Video-Input]
        \item[Datensatz]
        \item[Genauigkeit] NOCS Datensatz
        \begin{itemize*}
            \item $26,5\%$ \Gls{55cm}
            \item $64,9\%$ \Gls{IoU25}
            \item $28,5$ \Gls{Rerr}
            \item $8,3$ \Gls{Terr}
        \end{itemize*}
        Ergebnisse aus \Gls{AUC} Messung
        \begin{itemize*}
            \item ADD $35,07\%$
            \item ADD-S $41,88\%$
        \end{itemize*}
        \item[Ressourcen]
        \item[Laufzeit]
    \end{description*}

    \subsection{Neural Analysis-by-Synthesis}\cite{CategoryLevelObject}
    \begin{description*}
        \item[Modell]
        \item[Video-Input]
        \item[Datensatz]
        \item[Genauigkeit]
        \item[Ressourcen]
        \item[Laufzeit]
    \end{description*}

    \subsection{6-PACK}\cite{6pack}
    \begin{description*}
        \item[Modell] Kategorie-bezogene 3D Modellen
        \item[Video-Input]
        \item[Datensatz]
        \item[Genauigkeit] \Gls{NOCS} Datensatz:
        \begin{itemize*}
            \item $33,3\%$ \Gls{55cm}
            \item $94,2\%$ \Gls{IoU25}
            \item $16,0$ \Gls{Rerr}
            \item $3,5$ \Gls{Terr}
        \end{itemize*}
        Ergebnisse aus \Gls{AUC} Messung
        \begin{itemize*}
            \item ADD $-\%$
            \item ADD-S $-\%$
        \end{itemize*}
        \item[Ressourcen]
        \item[Laufzeit]
    \end{description*}

    \subsection{PoseCNN}\cite{PoseCNN}
    \begin{description*}
        \item[Modell]
        \item[Video-Input]
        \item[Datensatz]
        \item[Genauigkeit]
        \item[Ressourcen]
        \item[Laufzeit]
    \end{description*}

    Ein neues \Gls{cnn} für die 6D-Objektposenschätzung. PoseCNN schätzt die 3D-Verschiebung eines Objekts, indem es sein Zentrum im Bild lokalisiert und seinen Abstand zur Kamera vorhersagt. Die 3D-Rotation des Objekts wird durch Regression auf eine \Gls{quaternion} geschätzt. Dabei führt man eine neue Verlustfunktion ein, die es PoseCNN ermöglicht, symmetrische Objekte zu behandeln. Erreicht Ende-zu-Ende 6D Posenschätzung und ist sehr robust gegenüber Verdeckungen zwischen Objekten.

    PoseCNN entkoppelt die Schätzung von 3D-Rotation und 3D-Translation. Es schätzt die 3D-Verschiebung durch Lokalisierung des Objektzentrums und Vorhersage des Zentrumsabstands. Durch Regression jedes Pixels auf einen Einheitsvektor in Richtung des Objektzentrums kann das Zentrum unabhängig vom Maßstab robust geschätzt werden. Noch wichtiger ist, dass die Pixel das Objektzentrum auch dann wählen, wenn es von anderen Objekten verdeckt wird. Die 3D-Drehung wird durch Regression auf eine Quaternion-Darstellung vorhergesagt. Es werden zwei neue Verlustfunktionen für die Rotationsschätzung eingeführt, wobei der ShapeMatch-Verlust für symmetrische Objekte entwickelt wurde. Dadurch ist PoseCNN in der Lage, Okklusion und symmetrische Objekte in unübersichtlichen Szenen zu verarbeiten. Dies eröffnet den Weg zur Verwendung von Kameras mit einer Auflösung und einem Sichtfeld, die weit über die derzeit verwendeten Tiefenkamerasysteme hinausgehen. Wir stellen fest, dass SLOSS manchmal zu lokalen Minimums im Pose-Raum führt, ähnlich wie ICP. Es wäre interessant, in Zukunft einen effizienteren Umgang mit symmetrischen Objekten in der 6D-Positionsschätzung zu erforschen.

    \subsection{Robust Gaussian Filter\cite{GaussianFilter}}
    \begin{description*}
        \item[Modell]
        \item[Video-Input]
        \item[Datensatz]
        \item[Genauigkeit]
        \item[Ressourcen]
        \item[Laufzeit]
    \end{description*}

    \section{Vergleich verschiedener Verfahren}

    \section{Fazit}

\end{multicols*}

\medskip

\printglossary[title=Glossar]

\printbibliography[title=Literatur]

\end{document}